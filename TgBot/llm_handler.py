import os
import logging
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

BASE_URL = os.getenv("LLM_BASE_URL")
API_KEY = os.getenv("LLM_API_KEY")
MODEL = os.getenv("LLM_MODEL")

def generate_answer_from_context(question: str, context: str) -> str:
    if not API_KEY:
        logger.error("LLM_API_KEY not set in environment")
        return context

    client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ –∏ –ø–æ–º–æ—â–Ω–∏–∫ –º–µ—Ö–∞–Ω–∏–∫–∞.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
2. –î–∞—Ç—å —á—ë—Ç–∫–∏–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
3. –û–±—ä—è—Å–Ω–∏—Ç—å –ø—Ä–∏—á–∏–Ω—É –æ—à–∏–±–∫–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º
4. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã

–ü—Ä–∞–≤–∏–ª–∞:
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ —á—Ç–æ —ç—Ç–æ –∑–∞ –æ—à–∏–±–∫–∞, –∑–∞—Ç–µ–º —á—Ç–æ –¥–µ–ª–∞—Ç—å
- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
üîç [–ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã]
üîß [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è]
"""

    user_prompt = f"""–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:
{context}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        logger.info(f"Generating answer for question: {question[:50]}...")
        response = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            max_tokens=500,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
            temperature=0.3  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        )

        result = response.choices[0].message.content.strip()
        logger.info(f"Generated answer length: {len(result)} chars")
        return result

    except Exception as e:
        logger.error(f"Error generating answer: {e}")
        return f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –í–æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã:\n\n{context}"


def summarize_text(text: str) -> str:
    if not API_KEY:
        logger.error("LLM_API_KEY not set in environment")
        return text[:500] + "..." if len(text) > 500 else text

    client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

    system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—Ä–∞—Ç–∫–æ–º—É –∏–∑–ª–æ–∂–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–æ–≤.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –°–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
2. –í—ã–¥–µ–ª–∏—Ç—å –≥–ª–∞–≤–Ω—ã–µ –∏–¥–µ–∏ –∏ —Ñ–∞–∫—Ç—ã
3. –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –¥–µ—Ç–∞–ª–∏ –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
4. –ù–∞–ø–∏—Å–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ

–ü—Ä–∞–≤–∏–ª–∞:
- –°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç –≤ 3-5 —Ä–∞–∑
- –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π
- –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ
- –°–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:
üìù **–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:**
[2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—É—Ç—å—é]

"""

    user_prompt = f"""–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞: {text}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        logger.info(f"Summarizing text of length: {len(text)} chars")
        response = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            max_tokens=800,
            temperature=0.3
        )

        result = response.choices[0].message.content.strip()
        logger.info(f"Summary length: {len(result)} chars")
        return result

    except Exception as e:
        logger.error(f"Error summarizing text: {e}")
        # Fallback
        return f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ.\n\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):\n{text[:500]}..."